---
layout: post
title:  "机器学习初探"
date:   2025-04-05
desc: "Java 源码 机器学习"
keywords: "Java 源码 机器学习"
categories: [Java]
tags: [Java]
icon: icon-html
---

机器学习建模流程
> 数据驱动：监督学习;非监督学习<br/>
> 1、首先做数据处理，清洗；2、数据的特征工程，选择算法；3、模型训练，模型评估；4、模型优化


## 特征工程(重点)
> 特征工程（Feature Engineering）是机器学习过程中非常重要的一步，指的是通过对原始数据的处理、转换和构造，生成新的特征或选择有效的特征，从而提高模型的性能。简单来说，特征工程是将原始数据转换为可以更好地表示问题的特征形式，帮助模型更好地理解和学习数据中的规律。优秀的特征工程可以显著提高模型的表现；反之，忽视特征工程可能导致模型性能欠佳

常用的方法：

1）低方差过滤法

对于特征的选择，可以直接基于方差来判断，这是最简单的。低方差的特征意味着该特征的所有样本值几乎相同，对预测影响极小，可以将其去掉
```sh
from sklearn.feature_selection import VarianceThreshold

# 低方差过滤：删除方差低于 0.01 的特征
var_thresh = VarianceThreshold(threshold=0.01)
X_filtered = var_thresh.fit_transform(X)
```

2）相关系数法

通过计算特征与目标变量或特征之间的相关性，筛选出高相关性特征（与目标相关）或剔除冗余特征（特征间高度相关）。
```sh
#例如，现有一数据集包括不同渠道广告投放金额与销售额。
#使用pandas.DataFrame.corrwith(method="pearson")计算各个特征与标签间的皮尔逊相关系数。
#正相关：值接近1，说明特征随目标变量增加而增加。
#负相关：值接近-1，说明特征随目标变量增加而减少。
#无关：值接近0，说明特征和目标变量无明显关系。
import pandas as pd

advertising = pd.read_csv("data/advertising.csv")
advertising.drop(advertising.columns[0], axis=1, inplace=True)
advertising.dropna(inplace=True)
X = advertising.drop("Sales", axis=1)
y = advertising["Sales"]
# 计算皮尔逊相关系数
print(X.corrwith(y, method="pearson"))
# TV   0.782224
# Radio 0.576223
# Newspaper 0.228299
# dtype: float64
```

斯皮尔曼相关系数

斯皮尔曼相关系数（Spearman’s Rank Correlation Coefficient）的定义是等级变量之间的皮尔逊相关系数。用于衡量两个变量之间的单调关系，即当一个变量增加时，另一个变量是否总是增加或减少（不要求是线性关系）。适用于非线性关系或数据不符合正态分布的情况
```sh
#斯皮尔曼相关系数的取值范围为：
#：完全正相关（一个变量增加，另一个变量也总是增加）。
#：完全负相关（一个变量增加，另一个变量总是减少）。
#：无相关性。

import pandas as pd

#例如，现有一组每周学习时长与数学考试成绩的数据
# 每周学习时长
X = [[5], [8], [10], [12], [15], [3], [7], [9], [14], [6]]
# 数学考试成绩
y = [55, 65, 70, 75, 85, 50, 60, 72, 80, 58]
# 计算斯皮尔曼相关系数
X = pd.DataFrame(X)
y = pd.Series(y)
print(X.corrwith(y, method="spearman"))
# 0.987879
```

3）主成分分析（PCA）

主成分分析（Principal Component Analysis，PCA）是一种常用的降维技术，通过线性变换将高维数据投影到低维空间，同时保留数据的主要变化模式。
```sh
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

n_samples = 1000
# 第1个主成分方向
component1 = np.random.normal(0, 1, n_samples)
# 第2个主成分方向
component2 = np.random.normal(0, 0.2, n_samples)
# 第3个方向（噪声，方差较小）
noise = np.random.normal(0, 0.1, n_samples)
# 构造3维数据
X = np.vstack([component1 - component2, component1 + component2, component2 + noise]).T

# 标准化
scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)

# 应用PCA，将3维数据降维到2维
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_standardized)

# 可视化
# 转换前的3维数据可视化
fig = plt.figure(figsize=(12, 4))
ax1 = fig.add_subplot(121, projection="3d")
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c="g")
ax1.set_title("Before PCA (3D)")
ax1.set_xlabel("Feature 1")
ax1.set_ylabel("Feature 2")
ax1.set_zlabel("Feature 3")
# 转换后的2维数据可视化
ax2 = fig.add_subplot(122)
ax2.scatter(X_pca[:, 0], X_pca[:, 1], c="g")
ax2.set_title("After PCA (2D)")
ax2.set_xlabel("Principal Component 1")
ax2.set_ylabel("Principal Component 2")
plt.show()
```

## 模型评估和模型选择(重点)

1）损失函数（值越小效果越好）

对于模型一次预测结果的好坏，需要有一个度量标准。对于监督学习而言，给定一个输入X，选取的模型就相当于一个“决策函数”f，它可以输出一个预测结果f(X)，而真实的结果（标签）记为Y。f(X) 和Y之间可能会有偏差，我们就用一个损失函数（loss function）来度量预测偏差的程度，记作 L(Y,f(X))。

0-1损失函数

平方损失函数

绝对损失函数

对数似然损失函数

2）经验误差与泛化误差
>根据选取的损失函数，就可以计算出模型f(X)在训练集上的平均误差，称为训练误差，也被称作 经验误差（empirical error） 或 经验风险（empirical risk）<br/>
> 类似地，在测试数据集上平均误差，被称为测试误差或者 泛化误差（generalization error）。<br/>
>一般情况下对模型评估的策略，就是考察经验误差；当经验风险最小时，就认为取到了最优的模型。这种策略被称为 经验风险最小化（empirical risk minimization，ERM）

3)欠拟合与过拟合

拟合（Fitting）是指机器学习模型在训练数据上学习到规律并生成预测结果的过程。理想情况下，模型能够准确地捕捉训练数据的模式，并且在未见过的新数据（测试数据）上也有良好的表现；即模型具有良好的 泛化能力。

欠拟合（Underfitting）：是指模型在训练数据上表现不佳，无法很好地捕捉数据中的规律。这样的模型不仅在训练集上表现不好，在测试集上也同样表现差。
>产生原因：
模型复杂度不足：模型过于简单，无法捕捉数据中的复杂关系。 特征不足：输入特征不充分，或者特征选择不恰当，导致模型无法充分学习数据的模式。 训练不充分：训练过程中迭代次数太少，模型没有足够的时间学习数据的规律。
过强的正则化：正则化项设置过大，强制模型过于简单，导致模型无法充分拟合数据。<br/>
> 解决办法：
增加模型复杂度：选择更复杂的模型。
增加特征或改进特征工程：添加更多的特征或通过特征工程来创造更有信息量的特征。
增加训练时间：增加训练的迭代次数，让模型有更多机会去学习。
减少正则化强度：如果使用了正则化，尝试减小正则化的权重，以让模型更灵活。

过拟合（Overfitting）：是指模型在训练数据上表现得很好，但在测试数据或新数据上表现较差的情况。过拟合的模型对训练数据中的噪声或细节过度敏感，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，从而失去了泛化能力。
>产生原因：
模型复杂度过高：模型过于复杂，参数太多。
训练数据不足：数据集太小，模型能记住训练数据的细节，但无法泛化到新数据。
特征过多：特征太多，模型可能会“记住”数据中的噪声，而不是学到真正的规律。
训练过长：训练时间过长，导致模型学习到训练数据中的噪声，而非数据的真正规律。<br/>
解决办法：
减少模型复杂度：降低模型的参数数量、使用简化的模型或降维来减小模型复杂度。
增加训练数据：收集更多数据，或通过数据增强来增加训练数据的多样性。
使用正则化：引入L1、L2正则化，避免过度拟合训练数据。
交叉验证：使用交叉验证技术评估模型在不同数据集上的表现，以减少过拟合的风险。
早停：训练时，当模型的验证损失不再下降时，提前停止训练，避免过度拟合训练集。
```sh
#生成随机数,在-3，3中模拟 拟合sin(x)
import numpy as np
import matplotlib.pyplot as plt #绘图
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

plt.rcParams["font.sans-serif"] = ["KaiTi"]
plt.rcParams["axes.unicode_minus"] = False

def polynomial(x, degree):
    """构成多项式，返回 [x^1,x^2,x^3,...,x^n]"""
    return np.hstack([x**i for i in range(1, degree + 1)])

X = np.linspace(-3, 3, 300).reshape(-1, 1)
y = np.sin(X) + np.random.uniform(-0.5, 0.5, 300).reshape(-1, 1)
fig, ax = plt.subplots(1, 3, figsize=(15, 4))
ax[0].plot(X, y, "yo")
ax[1].plot(X, y, "yo")
ax[2].plot(X, y, "yo")

# 划分训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建线性回归模型
model = LinearRegression()

# 欠拟合
x_train1 = x_train
x_test1 = x_test
model.fit(x_train1, y_train) # 模型训练
y_pred1 = model.predict(x_test1) # 预测
ax[0].plot(np.array([[-3], [3]]), model.predict(np.array([[-3], [3]])), "c") # 绘制曲线
ax[0].text(-3, 1, f"测试集均方误差：{mean_squared_error(y_test, y_pred1):.4f}")
ax[0].text(-3, 1.3, f"训练集均方误差：{mean_squared_error(y_train, model.predict(x_train1)):.4f}")

# 恰好拟合
x_train2 = polynomial(x_train, 5)
x_test2 = polynomial(x_test, 5)
model.fit(x_train2, y_train) # 模型训练
y_pred2 = model.predict(x_test2) # 预测
ax[1].plot(X, model.predict(polynomial(X, 5)), "k") # 绘制曲线
ax[1].text(-3, 1, f"测试集均方误差：{mean_squared_error(y_test, y_pred2):.4f}")
ax[1].text(-3, 1.3, f"训练集均方误差：{mean_squared_error(y_train, model.predict(x_train2)):.4f}")

# 过拟合
x_train3 = polynomial(x_train, 20)
x_test3 = polynomial(x_test, 20)

model.fit(x_train3, y_train) # 模型训练
y_pred3 = model.predict(x_test3) # 预测
ax[2].plot(X, model.predict(polynomial(X, 20)), "r") # 绘制曲线
ax[2].text(-3, 1, f"测试集均方误差：{mean_squared_error(y_test, y_pred3):.4f}")
ax[2].text(-3, 1.3, f"训练集均方误差：{mean_squared_error(y_train, model.predict(x_train3)):.4f}")
plt.show()
```